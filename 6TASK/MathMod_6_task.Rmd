---
title: "6task"
author: "V.S. Romantseva"
date: '14 мая 2018 г '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# РЕГУЛЯРИЗАЦИЯ ЛИНЕЙНЫХ МОДЕЛЕЙ

Данные: Carseats {ISLR}

```{r, echo=FALSE, warning=FALSE, message=FALSE}
library('ISLR') # набор данных
library('leaps') # функция regsubset() — отбор оптимального 
# подмножества переменных
library('glmnet') # функция glmnet() — лассо
library('pls') # регрессия на главные компоненты — pcr()
# и частный МНК — plsr()

my.seed <- 1
train.percent <- 0.5
```

#Отбор оптимального подмножества

```{r}

# подгоняем модели с сочетаниями предикторов 
regfit.full <- regsubsets(Sales ~ ., Carseats)
summary(regfit.full)

# подгоняем модели с сочетаниями предикторов до 11 (максимум в данных)
regfit.full <- regsubsets(Sales ~ ., Carseats, nvmax = 11)
reg.summary <- summary(regfit.full)
reg.summary

# структура отчёта по модели 
names(reg.summary)
```

Ищем характеристики качесва моделей.

```{r}
# R^2 и скорректированный R^2
round(reg.summary$rsq, 3)

# на графике
plot(1:11, reg.summary$rsq, type = 'b',
     xlab = 'Количество предикторов', ylab = 'R-квадрат')
# сода же добавим скорректированный R-квадрат
points(1:11, reg.summary$adjr2, col = 'red')
# модель с максимальным скорректированным R-квадратом
which.max(reg.summary$adjr2)

### 10
points(which.max(reg.summary$adjr2), 
       reg.summary$adjr2[which.max(reg.summary$adjr2)],
       col = 'red', cex = 2, pch = 20)
legend('bottomright', legend = c('R^2', 'R^2_adg'),
       col = c('black', 'red'), lty = c(1, NA),
       pch = c(1, 1))

# C_p
reg.summary$cp

# число предикторов у оптимального значения критерия
which.min(reg.summary$cp)

### 7
# график
plot(reg.summary$cp, xlab = 'Число предикторов',
     ylab = 'C_p', type = 'b')
points(which.min(reg.summary$cp),
       reg.summary$cp[which.min(reg.summary$cp)], 
       col = 'red', cex = 2, pch = 20)

# BIC
reg.summary$bic

# число предикторов у оптимального значения критерия
which.min(reg.summary$bic)

### 7
# график
plot(reg.summary$bic, xlab = 'Число предикторов',
     ylab = 'BIC', type = 'b')
points(which.min(reg.summary$bic),
       reg.summary$bic[which.min(reg.summary$bic)], 
       col = 'red', cex = 2, pch = 20)

# метод plot для визуализации результатов

plot(regfit.full, scale = 'r2')

plot(regfit.full, scale = 'adjr2')

plot(regfit.full, scale = 'Cp')

plot(regfit.full, scale = 'bic')

# коэффициенты модели с наименьшим BIC
round(coef(regfit.full, 7), 3)


```

Лучшей является модель с 7 предикторами, вместо имеющихся 11. 


#Лассо
```{r}
# из-за синтаксиса glmnet() формируем явно матрицу объясняющих...
x <- model.matrix(Sales ~ ., Carseats)[, -1]

# и вектор значений зависимой переменной
y <- Carseats$Sales

set.seed(my.seed)
train <- sample(1:nrow(x), nrow(x)/2)
test <- -train
y.test <- y[test]
# вектор значений гиперпараметра лямбда
grid <- 10^seq(10, -2, length = 100)

lasso.mod <- glmnet(x[train, ], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)

set.seed(my.seed)
cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1)
plot(cv.out)

bestlam <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test, ])

#MSE на тестовой
round(mean((lasso.pred - y.test)^2), 0)

# коэффициенты лучшей модели
out <- glmnet(x, y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = 'coefficients',
                      s = bestlam)[1:11, ]
round(lasso.coef, 4)

round(lasso.coef[lasso.coef != 0], 4)

```

У обеих моделей показатели качества схожие. Однако mse лассо регрессии на 2 сотых меньше mse отбора оптимального подмножества.
