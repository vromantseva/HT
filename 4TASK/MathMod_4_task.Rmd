---
title: "4_task"
author: "V.S. Romantseva"
date: '18 марта 2018 г '
output: html_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Цель: исследовать набор данных Carseats с помощью линейной регрессионной модели. Задействовав все возможные регрессоры, сделать вывод о пригодности модели для прогноза. Сравнить с методом k ближайших соседей по MSE на тестовой выборке.

```{r, echo=FALSE}
library('GGally')       # графики совместного разброса переменных
library('lmtest')       # тесты остатков регрессионных моделей
library('FNN')          # алгоритм kNN
library('ISLR')

data(Carseats)
Carseats <- Carseats [, -2] 
Carseats <- Carseats [, -2] 
Carseats <- Carseats [, -3] 
Carseats <- Carseats [, -5] 
Carseats <- Carseats [, -5] 
Carseats <- Carseats [, -5] 
str(Carseats)
# константы
my.seed <- 12345
train.percent <- 0.85


# обучающая выборка
# обучающая выборка
set.seed(my.seed)
inTrain <- sample(seq_along(Carseats$Sales), 
                  nrow(Carseats) * train.percent)
df.train <- Carseats[inTrain,]
df.test <- Carseats[-inTrain,]

```

# Переменные
Sales - продажи детских автокресел в тысячах.

Prices - цена на каждое из автокресел.

Advertising - бюджет компаний на рекламу.

US - фактор, принимающий значения Yes, No, показывающий находится ли магазин в США или нет.


ShelveLoc - Фактор с уровнями Bad, Good и Medium, показывающий качество местоположения стеллажа для сидений автомобиля на каждом сайте

Размерность обучающей выборки: n=400 строк, p=4 объясняющих переменных. Зависимая переменная – Sales.

# Графики

```{r, echo=FALSE}
# совместный график разброса переменных
ggp <- ggpairs(df.train)
print(ggp, progress = F)

# цвета по фактору US

ggp <- ggpairs(df.train[, c('US', 'Advertising' , 'Price', 'Sales')], 
               mapping = ggplot2::aes(color = US), cardinality_threshold = 27)
print(ggp, progress = F)

# цвета по фактору ShelveLoc

ggp <- ggpairs(df.train[, c('ShelveLoc' , 'Advertising', 'Price', 'Sales')], 
               mapping = ggplot2::aes(color = ShelveLoc), cardinality_threshold = 27)
print(ggp, progress = F)

```

# Модели

```{r, echo=FALSE}
model.1 <- lm(Sales ~ . + Price:US + Price:ShelveLoc + Advertising:US + Advertising:ShelveLoc,
              data = df.train)
summary(model.1)
```

Убираем Advertising:ShelveLoc, т.к. коэффициенты при преременных являются наиболее незначимыми.

```{r, echo=FALSE}
model.2 <- lm(Sales ~ . + Price:US + Price:ShelveLoc + Advertising:US,
              data = df.train)
summary(model.2)
```

Убираем Advertising:US, т.к. коэффициент является наиболее незначимыми. В целом модель не стала лучше, перейде к следущему исключению регрессогов.

```{r, echo=FALSE}

model.3 <- lm(Sales ~ . + Price:US + Price:ShelveLoc ,
              data = df.train)
summary(model.3)

model.4 <- lm(Sales ~ . + Price:ShelveLoc,
              data = df.train)
summary(model.4)
```

Были убраны переменные Price:US и Price:ShelveLoc, т.к. они не влияют на Sales.

```{r, echo=FALSE}
model.5 <- lm(Sales ~ . ,
              data = df.train)
summary(model.5)
```

Переменная US является незначимой, построим уравнение без нее.

```{r, echo=FALSE}
model.6 <- lm(Sales ~ Advertising + Price + ShelveLoc ,
              data = df.train)
summary(model.6)
```

В полученной модели 6 все коэффициенты являются значимыми. Далее работать будем с ней.

# Проверка остатков

```{r, echo=FALSE}
# тест Бройша-Пагана
bptest(model.6)
# статистика Дарбина-Уотсона
dwtest(model.6)
# графики остатков
par(mar = c(4.5, 4.5, 2, 1))
par(mfrow = c(1, 3))
plot(model.4, 1)
plot(model.4, 4)
plot(model.4, 5)

par(mfrow = c(1, 1))
```



# Сравнение с kNN

```{r, echo=FALSE}
# фактические значения y на тестовой выборке
y.fact <- Carseats[-inTrain, 1]
y.model.lm <- predict(model.6, df.test)
MSE.lm <- sum((y.model.lm - y.fact)^2) / length(y.model.lm)

str(Carseats)

sh <- Carseats$ShelveLoc
str(sh)

k<-as.numeric(sh)
Carseats$ShelveLoc <- k

us1 <- Carseats$US
k1<-as.numeric(sh)
Carseats$US <- k1

df.train <- Carseats[inTrain,]
df.test <- Carseats[-inTrain,]

# kNN требует на вход только числовые переменные
df.train.num <- as.data.frame(df.train)
df.test.num <- as.data.frame(df.test)

for (i in 2:50){
  model.knn <- knn.reg(train = df.train.num[, !(colnames(df.train.num) %in% "Sales")], 
                       y = df.train.num[, "Sales"], 
                       test = df.test.num[, !(colnames(df.train.num) %in% "Sales")], k = i)
  y.model.knn <- model.knn$pred
  if (i == 2){
    MSE.knn <- sum((y.model.knn - y.fact)^2) / length(y.model.knn)
  } else {
    MSE.knn <- c(MSE.knn, 
                 sum((y.model.knn - y.fact)^2) / length(y.model.knn))
  }
}

# график
par(mar = c(4.5, 4.5, 1, 1))
plot(2:50, MSE.knn, type = 'b', col = 'darkgreen',
     xlab = 'значение k', ylab = 'MSE на тестовой выборке')
lines(2:50, rep(MSE.lm, 49), lwd = 2, col = grey(0.2), lty = 2)
legend('topright', lty = c(1, 2), pch = c(1, NA), 
       col = c('darkgreen', grey(0.2)), 
       legend = c('k ближайших соседа', 'регрессия (все факторы)'), 
       lwd = rep(2, 2))


```

#MSE полученное с помощью метода kNN больше, чем у модели линейной регрессии. Однако модель линейной регрессии объясняет всего 28% выборки, что показывает ее низкое качество. Качество модели можно улучшить при введении дополнительных переменных в уравнение.

