---
title: "1_task"
author: "Romantseva V.S."
date: '22 февраля 2018 г '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


###Оценка точности модели с непрерывной зависимой переменной
В практических примерах ниже показано:

1. Как делить данные на выборки (обучающую и тестовую);  
2. Как считать MSE: среднеквадратическую ошибку модели;  
3. Как меняются MSE на тестовой и обучающей выборках с изменением гибкости (числа степеней свободы)   модели.  
*Модели:* сглаживающие сплайны.  
*Данные:* сгенерированные.

Рассмотрим как меняется поведение ошибок на тестовой и обучающей выборках при различном числе степеней свободы, если функция зависимости отклика **Y** от единственного признака **X** известна. Сгенерируем **X** и **Y**:

 **X∼U(5,105)X∼U(5,105)  
 Y=f(X)+ϵY=f(X)+ϵ**  
 где f(x)=8+3.5*sin((x-30)/9), ϵ∼N(0,1)

```{r}
#  Генерируем данные ###########################################################

my.seed <- 1486372882    # ядро
n.all <- 60              # наблюдений всего
train.percent <- 0.85    # доля обучающей выборки
res.sd <- 1              # стандартное отклонение случайного шума
x.min <- 5               # границы изменения X: нижняя
x.max <- 105             #  и верхняя

# фактические значения x
set.seed(my.seed)
x <- runif(x.min, x.max, n = n.all)

# случайный шум
set.seed(my.seed)
res <- rnorm(mean = 0, sd = res.sd, n = n.all)

# отбираем наблюдения в обучающую выборку
set.seed(my.seed)
inTrain <- sample(seq_along(x), size = train.percent*n.all)

# истинная функция взаимосвязи
y.func <- function(x) {8+3.5*sin((x-30)/9)}

# для графика истинной взаимосвязи
x.line <- seq(x.min, x.max, length = n.all)
y.line <- y.func(x.line)

# фактические значения y (с шумом)
y <- y.func(x) + res

# Создаём векторы с данными для построения графиков ############################

# наблюдения на обучающей выборке
x.train <- x[inTrain]
y.train <- y[inTrain]

# наблюдения на тестовой выборке
x.test <- x[-inTrain]
y.test <- y[-inTrain]
```
Изобразим исходные данные на графике.
```{r}
#  График 1: Исходные данные на график #########################################

# убираем широкие поля рисунка
par(mar = c(4, 4, 1, 1))

# наименьшие/наибольшие значения по осям
x.lim <- c(x.min, x.max)
y.lim <- c(min(y), max(y))

# наблюдения с шумом (обучающая выборка)
plot(x.train, y.train, 
     col = grey(0.2), bg = grey(0.2), pch = 21,
     xlab = 'X', ylab = 'Y', 
     xlim = x.lim, ylim = y.lim, 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# наблюдения тестовой выборки
points(x.test, y.test,
       col = 'red', bg = 'red', pch = 21)

# истинная функция
lines(x.line, y.line,
      lwd = 2, lty = 2)
# заголовок
mtext('Исходные данные и истинная функция связи', side = 3)

# легенда
legend('bottomright', legend = c('обучение', 'тест', 'f(X)'),
       pch = c(16, 16, NA), 
       col = c(grey(0.2), 'red', 'black'),  
       lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2)
```
 
  В качестве модели используем сплайны со степенями свободы от 2 (прямая) до 40 (количество узлов равно 2/3 наблюдений). Строим модели с различным количеством степеней свободы и в каждом случае считаем среднеквадратическую ошибку модели на обучающей и тестовой выборках
  
```{r}
#  Строим модель №2 из лекции (df = 6) #########################################

# модель 2 (сплайн с df = 6)
mod <- smooth.spline(x = x.train, y = y.train, df = 6)

# модельные значения для расчёта ошибок
y.model.train <- predict(mod, data.frame(x = x.train))$y[, 1]
y.model.test <- predict(mod, data.frame(x = x.test))$y[, 1]

# считаем средний квадрат ошибки на обечающей и тестовой выборке
MSE <- c(sum((y.train - y.model.train)^2) / length(x.train),
         sum((y.test - y.model.test)^2) / length(x.test))
names(MSE) <- c('train', 'test')
round(MSE, 2)
```

```{r}
#  Теперь строим модели с df от 2 до 40 ########################################

# максимальное число степеней свободы для модели сплайна
max.df <- 40

tbl <- data.frame(df = 2:max.df)   # таблица для записи ошибок
tbl$MSE.train <- 0                 # столбец: ошибки на обучающей выборке
tbl$MSE.test <- 0                  # столбец: ошибки на тестовой выборке

# цикл по степеням свободы
for (i in 2:max.df) {
    # строим модель
    mod <- smooth.spline(x = x.train, y = y.train, df = i)
    
    # модельные значения для расчёта ошибок
    y.model.train <- predict(mod, data.frame(x = x.train))$y[, 1]
    y.model.test <- predict(mod, data.frame(x = x.test))$y[, 1]
    
    # считаем средний квадрат ошибки на обечающей и тестовой выборке
    MSE <- c(sum((y.train - y.model.train)^2) / length(x.train),
             sum((y.test - y.model.test)^2) / length(x.test))
    
    # записываем ошибки в модель
    tbl[tbl$df == i, c('MSE.train', 'MSE.test')] <- MSE
}

# первые строки таблицы
head(tbl)
```

Изобразим на графике поведение ошибок при различном количестве степеней свободы.

```{r}
#  График 2: Зависимость MSE от гибкости модели ################################

plot(tbl$df, tbl$MSE.test,
     type = 'l', col = 'red', lwd = 2,
     xlab = 'Степени свободы сплайна', ylab = 'MSE',
     ylim = c(min(tbl$MSE.train, tbl$MSE.test), 
              max(tbl$MSE.train, tbl$MSE.test)),
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

points(tbl$df, tbl$MSE.test,
       pch = 21, col = 'red', bg = 'red')

lines(tbl$df, tbl$MSE.train, 
      col = grey(0.3), lwd = 2)
# неустранимая ошибка
abline(h = res.sd,
       lty = 2, col = grey(0.4), lwd = 2)

# степени свободы у наименьшей ошибки на тестовой выборке
min.MSE.test <- min(tbl$MSE.test)
df.min.MSE.test <- 
  tbl[tbl$MSE.test == min.MSE.test,
      'df']

# сообщение в консоль
message(paste0('Наименьшая MSE на тестовой выборке равна ', 
               round(min.MSE.test, 2),  
               ' и достигается при df = ', df.min.MSE.test, '.'))

# компромисс между точностью и простотой модели по графику
df.my.MSE.test <- 6
my.MSE.test <- 
  tbl[tbl$df == df.my.MSE.test,
      'MSE.test']

# легенда
legend('topright', legend = c('обучающая', 'тестовая'),
       pch = c(NA, 16), 
       col = c(grey(0.2), 'red'),  
       lty = c(1, 1), lwd = c(2, 2), cex = 1.2)

# сообщение в консоль
message(paste0('Компромисс между точностью и сложностью модели при df = ', 
               df.my.MSE.test, ', MSE = ', round(my.MSE.test, 2), '.'))

# ставим точку на графике
abline(v = 6,
       lty = 2, lwd = 2)
points(6, my.MSE.test,
       pch = 15, col = 'blue')
mtext(df.my.MSE.test, 
      side = 1, line = -1, at = df.my.MSE.test, col = 'blue', cex = 1.2)

mtext('Изменение MSE с ростом числа степеней свободы', side = 3)
```

На этом графике:  

- При движении слева направо MSE на обучающей выборке (серая кривая) сокращается, потому что с ростом числа степеней свободы расчёт число узлов, по которым строится сплайн. При этом модельная кривая подгоняется по всё возрастающему количеству точек и становится всё более гибкой. В результате индивидуальные расстояния от фактических наблюдений за YY до их модельных оценок сокращаются, что приводит к сокращению MSE.
- При движении слева направо MSE на тестовой выборке (красная кривая) плавно растёт. Число степеней свободы такой модели равно числу оцениваемых параметров, т.е. 6 (коэффициенты перед X, и константа). Рост MSE на тестовой выборке объясняется эффектом переобучения модели: она всё лучше описывает обучающую выборку, и при этом постепенно становится неприменимой ни к одному другому набору наблюдений.
Наименьшее значение MSE на тестовой выборке соответствует числу степеней свободы 6 и равно 1.46. Это и есть компромисс между простотой и точностью модели.

График с моделью, выбранной в качестве лучшей, показан на рисунке ниже.

```{r}
#  График 3: Лучшая модель (компромисс между гибкостью и точностью) ############

mod.MSE.test <- 
  smooth.spline(x.train, y.train, 
                df = df.my.MSE.test)

# для гладких графиков модели
x.model.plot <- seq(x.min, x.max, length = 250)
y.model.plot <- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1]

# убираем широкие поля рисунка
par(mar = c(4, 4, 1, 1))

# наименьшие/наибольшие значения по осям
x.lim <- c(x.min, x.max)
y.lim <- c(min(y), max(y))

# наблюдения с шумом (обучающая выборка)
plot(x.train, y.train,
     col = grey(0.2), bg = grey(0.2), pch = 21,
     xlab = 'X', ylab = 'Y', 
     xlim = x.lim, ylim = y.lim, 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# наблюдения тестовой выборки
points(x.test, y.test,
       col = 'red', bg = 'red', pch = 21)

# истинная функция
lines(x.line, y.line,
      lwd = 2, lty = 2)

# модель
lines(x.model.plot, y.model.plot,
      lwd = 2, col = 'blue')

# заголовок
mtext('Исходные данные и лучшая модель', side = 3)

# легенда
legend('bottomright', legend = c('обучение', 'тест', 'f(X)', 'модель'),
       pch = c(16, 16, NA, NA), 
       col = c(grey(0.2), 'red', 'black', 'blue'),  
       lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2)
```

Теперь изменим стандартное отклонение случайного шума с 1 до 1,5.
Ниже можно увидеть графики, описывающие изменение моделей.

```{r pressure, echo=FALSE}
#  Генерируем данные ###########################################################

# ядро
my.seed <- 1486372882

# наблюдений всего
n.all <- 60
# доля обучающей выборки
train.percent <- 0.85
# стандартное отклонение случайного шума
res.sd <- 1.5
# границы изменения X
x.min <- 5
x.max <- 105

# фактические значения x
set.seed(my.seed)
x <- runif(x.min, x.max, n = n.all)


# случайный шум
set.seed(my.seed)
res <- rnorm(0, res.sd, n = n.all)


# отбираем наблюдения в обучающую выборку
set.seed(my.seed)
inTrain <- sample(seq_along(x), 
                  size = n.all*train.percent)


# истинная функция взаимосвязи 
y.func <- function(x) {8+3.5*sin((x-30)/9)}

# для графика истинной взаимосвязи
x.line <- seq(x.min, x.max, length = n.all)
y.line <- y.func(x.line)

# фактические значения y (с шумом)
y <- y.func(x) + res


# Создаём векторы с данными для построения графиков ############################

# наблюдения на обучающей выборке
x.train <- x[inTrain]
y.train <- y[inTrain]

# наблюдения на тестовой выборке
x.test <- x[-inTrain]
y.test <- y[-inTrain]

#  График 1: Исходные данные на график #########################################

# убираем широкие поля рисунка
par(mar = c(4, 4, 1, 1))

# наименьшие/наибольшие значения по осям
x.lim <- c(x.min, x.max)
y.lim <- c(min(y), max(y))

# наблюдения с шумом (обучающая выборка)
plot(x.train, y.train, 
     col = grey(0.2), bg = grey(0.2), pch = 21,
     xlab = 'X', ylab = 'Y', 
     xlim = x.lim, ylim = y.lim, 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# наблюдения тестовой выборки
points(x.test, y.test,
       col = 'red', bg = 'red', pch = 21)

# истинная функция
lines(x.line, y.line,
      lwd = 2, lty = 2)
# заголовок
mtext('Исходные данные и истинная функция связи', side = 3)

# легенда
legend('bottomright', legend = c('обучение', 'тест', 'f(X)'),
       pch = c(16, 16, NA), 
       col = c(grey(0.2), 'red', 'black'),  
       lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2)

#  Строим модель ###############################################################

# модель 2 (сплайн с df = 6)
mod <- smooth.spline(x = x.train, y = y.train, df = 6)

# модельные значения для расчёта ошибок
y.model.train <- predict(mod, 
                         data.frame(x = x.train))$y[, 1]

y.model.test <- predict(mod, 
                        data.frame(x = x.test))$y[, 1]


# считаем средний квадрат ошибки на обучающей и тестовой выборке
MSE <- c(mean((y.train - y.model.train)^2),
         mean((y.test - y.model.test)^2))
names(MSE) <- c('train', 'test')


#  Цикл по степеням свободы ####################################################

# максимальное число степеней свободы для модели сплайна
max.df <- 40
# таблица для записи ошибок
tbl <- data.frame(df = 2:max.df)
# ошибки на обучающей выборке
tbl$MSE.train <- 0
# ошибки на тестовой выборке
tbl$MSE.test <- 0


for (i in 2:max.df) {
  # модель
  mod <- smooth.spline(x = x.train, 
                       y = y.train, 
                       df = i)
  
  # модельные значения для расчёта ошибок
  y.model.train <- 
    predict(mod, 
            data.frame(x = x.train))$y[, 1]
  y.model.test <- 
    predict(mod, 
            data.frame(x = x.test))$y[, 1]
  
  # считаем средний квадрат ошибки на обучающей и тестовой выборке
  MSE <- c(mean((y.train - y.model.train)^2),
           mean((y.test - y.model.test)^2))
  
  # записываем результат в таблицу
  tbl[tbl$df == i, c('MSE.train', 'MSE.test')] <- MSE
}

#  График 2: Зависимость MSE от гибкости модели ################################

plot(tbl$df, tbl$MSE.test,
     type = 'l', col = 'red', lwd = 2,
     xlab = 'Степени свободы сплайна', ylab = 'MSE',
     ylim = c(min(tbl$MSE.train, tbl$MSE.test), 
              max(tbl$MSE.train, tbl$MSE.test)),
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

points(tbl$df, tbl$MSE.test,
       pch = 21, col = 'red', bg = 'red')

lines(tbl$df, tbl$MSE.train, 
      col = grey(0.3), lwd = 2)
# неустранимая ошибка
abline(h = res.sd,
       lty = 2, col = grey(0.4), lwd = 2)

# степени свободы у наименьшей ошибки на тестовой выборке
min.MSE.test <- min(tbl$MSE.test)
df.min.MSE.test <- 
  tbl[tbl$MSE.test == min.MSE.test,
      'df']

# сообщение в консоль
message(paste0('Наименьшая MSE на тестовой выборке равна ', 
               round(min.MSE.test, 2),  
               ' и достигается при df = ', df.min.MSE.test, '.'))

# компромисс между точностью и простотой модели по графику
df.my.MSE.test <- 6
my.MSE.test <- 
  tbl[tbl$df == df.my.MSE.test,
      'MSE.test']

# легенда
legend('topright', legend = c('обучающая', 'тестовая'),
       pch = c(NA, 16), 
       col = c(grey(0.2), 'red'),  
       lty = c(1, 1), lwd = c(2, 2), cex = 1.2)

# сообщение в консоль
message(paste0('Компромисс между точностью и сложностью модели при df = ', 
               df.my.MSE.test, ', MSE = ', round(my.MSE.test, 2), '.'))

# ставим точку на графике
abline(v = 6,
       lty = 2, lwd = 2)
points(6, my.MSE.test,
       pch = 15, col = 'blue')
mtext(df.my.MSE.test, 
      side = 1, line = -1, at = df.my.MSE.test, col = 'blue', cex = 1.2)

mtext('Изменение MSE с ростом числа степеней свободы', side = 3)

#  График 3: Лучшая модель (компромисс между гибкостью и точностью) ############

mod.MSE.test <- 
  smooth.spline(x.train, y.train, 
                df = df.my.MSE.test)

# для гладких графиков модели
x.model.plot <- seq(x.min, x.max, length = 250)
y.model.plot <- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1]

# убираем широкие поля рисунка
par(mar = c(4, 4, 1, 1))

# наименьшие/наибольшие значения по осям
x.lim <- c(x.min, x.max)
y.lim <- c(min(y), max(y))

# наблюдения с шумом (обучающая выборка)
plot(x.train, y.train,
     col = grey(0.2), bg = grey(0.2), pch = 21,
     xlab = 'X', ylab = 'Y', 
     xlim = x.lim, ylim = y.lim, 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# наблюдения тестовой выборки
points(x.test, y.test,
       col = 'red', bg = 'red', pch = 21)

# истинная функция
lines(x.line, y.line,
      lwd = 2, lty = 2)

# модель
lines(x.model.plot, y.model.plot,
      lwd = 2, col = 'blue')

# заголовок
mtext('Исходные данные и лучшая модель', side = 3)

# легенда
legend('bottomright', legend = c('обучение', 'тест', 'f(X)', 'модель'),
       pch = c(16, 16, NA, NA), 
       col = c(grey(0.2), 'red', 'black', 'blue'),  
       lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2)

```
При сравнении моделей можно заметить, что ошибка первой модели (1,46) меньше ошибки второй модели (3,23). Увеличение MSE вызвано увеличением стандартного отклонения случайного шума с 1 до 1,5. Можно сделать вывод, что первая модель имеет лучшие характеристика качества.
